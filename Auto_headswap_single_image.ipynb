{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto headswap single image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAaplCrIpFYWh+uof/0Jy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijishmadhavan/Auto-Head-Swap/blob/master/Auto_headswap_single_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NwO56RLg4Bx"
      },
      "outputs": [],
      "source": [
        "#@title Inputs { display-mode: \"form\" }\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import files\n",
        "import time\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from skimage.color import rgb2hsv, hsv2rgb\n",
        "from cv2.ximgproc import guidedFilter\n",
        "from PIL import ImageFilter\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,10)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "!mkdir -p \"/content/output\"\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import urllib.request as urlreq\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from pylab import rcParams\n",
        "from google.colab.patches import cv2_imshow\n",
        "%matplotlib inline\n",
        "\n",
        "haarcascade_url = \"1NXCKbuafQZekUstRytk1VO9fpjwujvJa\"\n",
        "haarcascade = \"haarcascade_frontalface_alt2.xml\"\n",
        "!wget https://www.dropbox.com/s/0gg2t150j8kx285/haarcascade_frontalface_alt_tree.xml\n",
        "\n",
        "if (haarcascade in os.listdir(os.curdir)):\n",
        "  print(\"File exists\")\n",
        "else:\n",
        "  !gdown --id $haarcascade_url --quiet\n",
        "  print(\"File downloaded\")\n",
        "  \n",
        "LBFmodel_url = \"147T_RQSD7_EoT5TROMox8qrd-mFt6UqZ\"\n",
        "LBFmodel = \"lbfmodel.yaml\"\n",
        "\n",
        "if (LBFmodel in os.listdir(os.curdir)):\n",
        "    print(\"File exists\")\n",
        "else:\n",
        "    !gdown --id $LBFmodel_url --quiet\n",
        "    print(\"File downloaded\")\n",
        "\n",
        "def show_landmarks(landmarks, image):\n",
        "  print(f\"Detected {len(landmarks)} faces. \")\n",
        "  img_show = cv2.imread(image)\n",
        "  for landmark in landmarks:\n",
        "    for i,(x,y) in enumerate(landmark[0]):\n",
        "\n",
        "      cv2.circle(img_show, (x,y),2,(255,255,255),2)\n",
        "      img_show = cv2.putText(img_show, f\"{i}\", (int(x-5),int(y-5)), cv2.FONT_HERSHEY_SIMPLEX,  \n",
        "                   0.5, (255, 0, 0) , 1, cv2.LINE_AA)\n",
        "\n",
        "  cv2_imshow(img_show)\n",
        "\n",
        "def landmarks(s_paths, show_lm=True):\n",
        "\n",
        "  landmarks_arr = []\n",
        "  detector = cv2.CascadeClassifier(haarcascade)\n",
        "\n",
        "  landmark_detector = cv2.face.createFacemarkLBF()\n",
        "  landmark_detector.loadModel(LBFmodel)\n",
        "\n",
        "  for s_path in s_paths:\n",
        "    image = cv2.imread(s_path)[...,::-1]\n",
        "    image_landmarks = image.copy()\n",
        "    image_draw = image.copy()\n",
        "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    faces = detector.detectMultiScale(image_gray)\n",
        "    \n",
        "    _, landmarks = landmark_detector.fit(image_gray, faces)\n",
        "    landmarks_arr.append(landmarks)\n",
        "  \n",
        "  if show_lm:\n",
        "    for i, s_path in enumerate(s_paths):\n",
        "      show_landmarks(landmarks_arr[i], s_path)\n",
        "\n",
        "  return landmarks_arr\n",
        "\n",
        "def landmarks_loaded(loaded_imgs, show_lm=True):\n",
        "  landmarks_arr = []\n",
        "\n",
        "  detector = cv2.CascadeClassifier(haarcascade)\n",
        "  landmark_detector = cv2.face.createFacemarkLBF()\n",
        "  landmark_detector.loadModel(LBFmodel)\n",
        "\n",
        "  for ld_img in loaded_imgs:\n",
        "    image = ld_img\n",
        "    image_landmarks = image.copy()\n",
        "    image_draw = image.copy()\n",
        "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = detector.detectMultiScale(image_gray)\n",
        "    _, landmarks = landmark_detector.fit(image_gray, faces)\n",
        "    landmarks_arr.append(landmarks)\n",
        "\n",
        "  return landmarks_arr\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as modelzoo\n",
        "\n",
        "resnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_chan, out_chan, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_chan)\n",
        "        self.conv2 = conv3x3(out_chan, out_chan)\n",
        "        self.bn2 = nn.BatchNorm2d(out_chan)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = None\n",
        "        if in_chan != out_chan or stride != 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_chan, out_chan,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_chan),\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = F.relu(self.bn1(residual))\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        shortcut = x\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "\n",
        "        out = shortcut + residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def create_layer_basic(in_chan, out_chan, bnum, stride=1):\n",
        "    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n",
        "    for i in range(bnum-1):\n",
        "        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class Resnet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Resnet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n",
        "        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n",
        "        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n",
        "        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        feat8 = self.layer2(x) # 1/8\n",
        "        feat16 = self.layer3(feat8) # 1/16\n",
        "        feat32 = self.layer4(feat16) # 1/32\n",
        "        return feat8, feat16, feat32\n",
        "\n",
        "    def init_weight(self):\n",
        "        state_dict = modelzoo.load_url(resnet18_url)\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            if 'fc' in k: continue\n",
        "            self_state_dict.update({k: v})\n",
        "        self.load_state_dict(self_state_dict)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module,  nn.BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_chan,\n",
        "                out_chan,\n",
        "                kernel_size = ks,\n",
        "                stride = stride,\n",
        "                padding = padding,\n",
        "                bias = False)\n",
        "        self.bn = nn.BatchNorm2d(out_chan)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.relu(self.bn(x))\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "class BiSeNetOutput(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n",
        "        super(BiSeNetOutput, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(AttentionRefinementModule, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n",
        "        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n",
        "        self.bn_atten = nn.BatchNorm2d(out_chan)\n",
        "        self.sigmoid_atten = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv_atten(atten)\n",
        "        atten = self.bn_atten(atten)\n",
        "        atten = self.sigmoid_atten(atten)\n",
        "        out = torch.mul(feat, atten)\n",
        "        return out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "\n",
        "class ContextPath(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(ContextPath, self).__init__()\n",
        "        self.resnet = Resnet18()\n",
        "        self.arm16 = AttentionRefinementModule(256, 128)\n",
        "        self.arm32 = AttentionRefinementModule(512, 128)\n",
        "        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
        "        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H0, W0 = x.size()[2:]\n",
        "        feat8, feat16, feat32 = self.resnet(x)\n",
        "        H8, W8 = feat8.size()[2:]\n",
        "        H16, W16 = feat16.size()[2:]\n",
        "        H32, W32 = feat32.size()[2:]\n",
        "\n",
        "        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n",
        "        avg = self.conv_avg(avg)\n",
        "        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n",
        "\n",
        "        feat32_arm = self.arm32(feat32)\n",
        "        feat32_sum = feat32_arm + avg_up\n",
        "        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n",
        "        feat32_up = self.conv_head32(feat32_up)\n",
        "\n",
        "        feat16_arm = self.arm16(feat16)\n",
        "        feat16_sum = feat16_arm + feat32_up\n",
        "        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n",
        "        feat16_up = self.conv_head16(feat16_up)\n",
        "\n",
        "        return feat8, feat16_up, feat32_up  # x8, x8, x16\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class SpatialPath(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(SpatialPath, self).__init__()\n",
        "        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n",
        "        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n",
        "        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n",
        "        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv1(x)\n",
        "        feat = self.conv2(feat)\n",
        "        feat = self.conv3(feat)\n",
        "        feat = self.conv_out(feat)\n",
        "        return feat\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class FeatureFusionModule(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
        "        super(FeatureFusionModule, self).__init__()\n",
        "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
        "        self.conv1 = nn.Conv2d(out_chan,\n",
        "                out_chan//4,\n",
        "                kernel_size = 1,\n",
        "                stride = 1,\n",
        "                padding = 0,\n",
        "                bias = False)\n",
        "        self.conv2 = nn.Conv2d(out_chan//4,\n",
        "                out_chan,\n",
        "                kernel_size = 1,\n",
        "                stride = 1,\n",
        "                padding = 0,\n",
        "                bias = False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, fsp, fcp):\n",
        "        fcat = torch.cat([fsp, fcp], dim=1)\n",
        "        feat = self.convblk(fcat)\n",
        "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
        "        atten = self.conv1(atten)\n",
        "        atten = self.relu(atten)\n",
        "        atten = self.conv2(atten)\n",
        "        atten = self.sigmoid(atten)\n",
        "        feat_atten = torch.mul(feat, atten)\n",
        "        feat_out = feat_atten + feat\n",
        "        return feat_out\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params = [], []\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "                wd_params.append(module.weight)\n",
        "                if not module.bias is None:\n",
        "                    nowd_params.append(module.bias)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nowd_params += list(module.parameters())\n",
        "        return wd_params, nowd_params\n",
        "\n",
        "\n",
        "class BiSeNet(nn.Module):\n",
        "    def __init__(self, n_classes, *args, **kwargs):\n",
        "        super(BiSeNet, self).__init__()\n",
        "        self.cp = ContextPath()\n",
        "        self.ffm = FeatureFusionModule(256, 256)\n",
        "        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n",
        "        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n",
        "        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.size()[2:]\n",
        "        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n",
        "        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n",
        "        feat_fuse = self.ffm(feat_sp, feat_cp8)\n",
        "\n",
        "        feat_out = self.conv_out(feat_fuse)\n",
        "        feat_out16 = self.conv_out16(feat_cp8)\n",
        "        feat_out32 = self.conv_out32(feat_cp16)\n",
        "\n",
        "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
        "        return feat_out, feat_out16, feat_out32\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
        "        for name, child in self.named_children():\n",
        "            child_wd_params, child_nowd_params = child.get_params()\n",
        "            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n",
        "                lr_mul_wd_params += child_wd_params\n",
        "                lr_mul_nowd_params += child_nowd_params\n",
        "            else:\n",
        "                wd_params += child_wd_params\n",
        "                nowd_params += child_nowd_params\n",
        "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n",
        "\n",
        "\n",
        "bisenet_fn = \"bisenet_pretrained.pth\"\n",
        "bisenet_url = \"1X_oBCBkhSi2W3lA_sOd_5P_M0I00idnv\"\n",
        "\n",
        "if (bisenet_fn in os.listdir(os.curdir)):\n",
        "  print(\"File exists\")\n",
        "else:\n",
        "  !gdown --id $bisenet_url --quiet\n",
        "  print(\"File downloaded\")\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "\n",
        "pretrained_W = \"/content/bisenet_pretrained.pth\"\n",
        "\n",
        "def face2parsing_maps(img_path):\n",
        "  \"\"\"\n",
        "    Parsing of the img_path image\n",
        "  \"\"\"\n",
        "  n_classes = 19\n",
        "  net = BiSeNet(n_classes=n_classes)\n",
        "  net.cpu()\n",
        "  net.load_state_dict(torch.load(pretrained_W, map_location = torch.device('cpu')))\n",
        "  net.eval()\n",
        "\n",
        "  to_tensor = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "  ])\n",
        "  with torch.no_grad():\n",
        "      img = Image.open(img_path)\n",
        "      \n",
        "      image = img.resize((512, 512), Image.BILINEAR)\n",
        "\n",
        "      img = to_tensor(image)\n",
        "      img = torch.unsqueeze(img, 0)\n",
        "      img = img.cpu()\n",
        "      out = net(img)[0]\n",
        "      parsing = out.squeeze(0).cpu().numpy().argmax(0)\n",
        "\n",
        "  return parsing\n",
        "def parsing2mask(im, parsing_anno, include=[0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "      im is a cv2 image\n",
        "      parsing_anno is the parsing returned by face2parsing_maps\n",
        "    \"\"\"\n",
        "    points = [[],[]]\n",
        "    vis_im = im.copy().astype(np.uint8)\n",
        "    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n",
        "    num_of_class = np.max(vis_parsing_anno)\n",
        "    mask = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 1))\n",
        "    \n",
        "    for pi in range(1, num_of_class + 1):\n",
        "        index = np.where(vis_parsing_anno == pi)\n",
        "\n",
        "        points[0].append(index[0])\n",
        "        points[1].append(index[1])\n",
        "\n",
        "        mask[index[0], index[1], :] = include[pi]\n",
        "\n",
        "    crop = vis_im.copy()\n",
        "    \n",
        "    mask_resized = cv2.resize(mask,(im.shape[1], im.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "    bitws = cv2.bitwise_and(crop,crop,mask=np.uint8(mask_resized))\n",
        "\n",
        "    return bitws, mask_resized, points\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Crop { display-mode: \"form\" }\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from cv2.ximgproc import guidedFilter\n",
        "from matplotlib import cm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "def get_masked_blur(mask, FM, kernel_s, iteration=1):\n",
        "  kernel = np.ones(kernel_s,np.uint8)\n",
        "\n",
        "  mask_im = np.array([mask,mask,mask]).transpose((1,2,0))\n",
        "  mask_im = cv2.erode(mask_im, kernel, iteration)  \n",
        "  \n",
        "\n",
        "  mask_im = (cv2.GaussianBlur(mask_im, (FM,FM), 0) > 0) * 1.0\n",
        "  mask_im = cv2.GaussianBlur(mask_im, (FM,FM), 0)\n",
        "  return mask_im  \n",
        "\n",
        "def add_margin(pil_img, top, right, bottom, left, color):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "\n",
        "def ref(path):\n",
        "  #path = \"'\" + path +\"'\"\n",
        "  cascPath = \"/content/haarcascade_frontalface_alt2.xml\"\n",
        "  faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "\n",
        "  im = Image.open(path)\n",
        "  im.save(\"test.jpg\")\n",
        "  a = max(im.getcolors(im.size[0]*im.size[1]))\n",
        "  image = cv2.imread(path)\n",
        "  image_crop = Image.open(path)\n",
        "  image_crop = image_crop.convert('RGB')\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor=1.1,\n",
        "    minNeighbors=20,\n",
        "    minSize=(30, 30),\n",
        "    flags = cv2.CASCADE_SCALE_IMAGE)\n",
        "  for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image, (x-w, y-h), (x+w+w, y+h+h), (0, 255, 0), 2)\n",
        "  im_crop = image_crop.crop((x-w, y-h, (x+w+w), (y+h+h)))\n",
        "  im_crop.save(\"rf_template.jpg\")\n",
        "  source_path = \"rf_template.jpg\"\n",
        "  source_im = cv2.imread(source_path)\n",
        "  parsing = face2parsing_maps(source_path)\n",
        "  masked, mask, points = parsing2mask(source_im, parsing,include=[0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0])\n",
        "  mask_blur = get_masked_blur(mask, 15, (15,15))\n",
        "  eps = 5e-6\n",
        "  eps *= 255*255\n",
        "  mask_face = guidedFilter(source_im.astype(np.float32), mask.astype(np.float32), 10, eps) \n",
        "  im = Image.fromarray(np.uint8(cm.gist_earth(mask_face)*255))\n",
        "  im.save(\"test1.png\")\n",
        "  flags = cv2.INPAINT_TELEA\n",
        "  flags2 = cv2.INPAINT_NS\n",
        "  mask = cv2.imread(\"test1.png\")\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  output = cv2.inpaint(source_im, mask, 55, flags=flags)\n",
        "  output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "  Image2 = Image.fromarray(output)\n",
        "  Image2.save(\"hl_template.jpg\")\n",
        "  Image1copy = image_crop.copy()\n",
        "  #Image2 = Image.open(path)\n",
        "  Image2copy = Image2.copy()\n",
        "  Image1copy.paste(Image2copy, (x-w, y-h))\n",
        "  #Image1copy.save('nohead.jpg')\n",
        "\n",
        "def source(path):\n",
        "  #path = \"'\" + path +\"'\"\n",
        "  cascPath = \"/content/haarcascade_frontalface_alt2.xml\"\n",
        "  faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "  im = Image.open(path)\n",
        "  image = cv2.imread(path)\n",
        "  image_crop = Image.open(path)\n",
        "  image_crop = image_crop.convert('RGB')\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor=1.1,\n",
        "    minNeighbors=20,\n",
        "    minSize=(30, 30),\n",
        "    flags = cv2.CASCADE_SCALE_IMAGE)\n",
        "  for (x, y, w, h) in faces:\n",
        "    #cv2.rectangle(image, (60, 0), (x+w+90, y+h+90), (0, 255, 0), 2)\n",
        "    cv2.rectangle(image, (x-w, y-h), (x+w+w, y+h+h), (0, 255, 0), 2)\n",
        "  if len(faces)<1:\n",
        "    im.save(\"source.jpg\")\n",
        "  else:\n",
        "    im_crop = image_crop.crop((x-w, y-h, (x+w+w), (y+h+h)))\n",
        "    im_crop.save(\"source.jpg\")\n",
        "\n",
        "\n",
        "def source_p(path):\n",
        "    im = Image.open(path)\n",
        "    return im.save(\"source.jpg\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L8Wa1rcShDbc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#headless template - Replace the head with source image Full body\n",
        "ref(\"/content/76894646.webp\")\n",
        "#source image Full body\n",
        "source(\"/content/ma.jpeg\")\n"
      ],
      "metadata": {
        "id": "WAKXUU_mhH3Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Output { display-mode: \"form\" }\n",
        "\n",
        "toggle_custom_name = True \n",
        "custom_source_name = \"source.jpg\" \n",
        "\n",
        "styles_arr = ['royal_1','royal_2','royal_3','royal_4','cather_5e6','choosing_crop','choosing5e5','courage_crop','Integrity_400','Integrity_crop','Integrity8e5','Interplay_m','Interplay','Justice_7e5','Lucian2','MeaningF_crop','MeaningF_crop2','MeaningF_m_488','MeaningF5e5','Mike_7e5','Miller_7e5','Miller_244','Miller_488','Mindseye5e5','Paul_7e5','Paul_crop_7e5','Tail_7e5','UdnieSS']\n",
        "style_selector = \"royal_1\" \n",
        "style_id = styles_arr.index(style_selector) + 1\n",
        "\n",
        "\n",
        "do_correction_filters = False \n",
        "double_color_match = False \n",
        "contrast_ = 0.2 \n",
        "saturation_ = 0.05 \n",
        "brightness_ = -0.05 \n",
        "\n",
        "\n",
        "do_automatic_color = False \n",
        "auto_color_intensity_ = 5.1\n",
        "color_variation_ = 10\n",
        "color_var = color_variation_/1e6\n",
        "\n",
        "\n",
        "do_hue_filter = False \n",
        "hue_shifter_ = 220.5 \n",
        "hue_intensity_ = 12.6 \n",
        "\n",
        "\n",
        "if toggle_custom_name:\n",
        "  source_path = f\"/content/{custom_source_name}\"\n",
        "else:\n",
        "  source_path = \"/content/source.jpg\"\n",
        "\n",
        "\n",
        "source_name = os.path.split(source_path)[-1]\n",
        "refTemp_path = \"/content/rf_template.jpg\"\n",
        "hless_path  = \"/content/hl_template.jpg\"\n",
        "\n",
        "\n",
        "cropface_path = f\"/content/output/crop_source.jpg\"\n",
        "cropfacestyle_path = \"/content/output/cropface_style.jpg\"\n",
        "cropfacestyleCT_path =\"/content/output/cropface_styleCT.jpg\"\n",
        "\n",
        "\n",
        "final_image_path = \"/content/output/final_image.jpg\"\n",
        "\n",
        "source_im = cv2.imread(source_path)\n",
        "refTemp_im = cv2.imread(refTemp_path)\n",
        "hlessTemp_im = cv2.imread(hless_path)\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.title('Source')\n",
        "plt.imshow(source_im[...,::-1])\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.title('Reference Teamplate')\n",
        "plt.imshow(refTemp_im[...,::-1])\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.title('Headless Teamplate')\n",
        "plt.imshow(hlessTemp_im[...,::-1])\n",
        "\n",
        "print('Inputs')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def draw_facebox(filename, dim, fill):\n",
        "    x,y,w,h = dim\n",
        "    data = plt.imread(filename)\n",
        "    plt.imshow(data)\n",
        "    ax = plt.gca()\n",
        "    x, y, width, height = dim\n",
        "    rect = plt.Rectangle((x-fill/2, y-fill/2,), width+fill, height+fill, fill=False, color='orange')\n",
        "    ax.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "def cropface(image, box, fill=50):\n",
        "  shape = image.shape\n",
        "  if len(shape) > 2 :\n",
        "    h,w,c = shape\n",
        "  else:\n",
        "    h,w = shape\n",
        "\n",
        "    print(mask)\n",
        "\n",
        "  x,y,w,h = box\n",
        "\n",
        "  y_fill = y-(fill//2) if y-(fill//2) > 0 else 0\n",
        "  x_fill = x-(fill//2) if x-(fill//2) > 0 else 0\n",
        "\n",
        "  h_fill = y+h+fill \n",
        "  w_fill = x+w+fill\n",
        "\n",
        "  return image[y_fill:(h_fill),(x_fill):(w_fill)]\n",
        "\n",
        "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    if width is None:\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "\n",
        "    else:\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\n",
        "\n",
        "    return resized\n",
        "\n",
        "parsing = face2parsing_maps(source_path)\n",
        "masked, mask, points = parsing2mask(source_im, parsing)\n",
        "\n",
        "box = cv2.boundingRect(mask.astype(np.uint8))\n",
        "cropface_img = cropface(source_im, box, fill=50)\n",
        "\n",
        "if cropface_img.shape[1] > 1000:\n",
        "  crop_r = image_resize(cropface_img, width = 1000)\n",
        "  print(\"Resizing Face\")\n",
        "else:\n",
        "  crop_r = cropface_img\n",
        "  \n",
        "plt.imsave(cropface_path, crop_r[...,::-1])\n",
        "\n",
        "\n",
        "crop_im = cv2.imread(cropface_path)\n",
        "\n",
        "parsing_c = face2parsing_maps(cropface_path)\n",
        "masked_c, mask_c, points = parsing2mask(crop_im, parsing_c)\n",
        "\n",
        "nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(mask_c.astype(np.uint8), connectivity=8)\n",
        "sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
        "\n",
        "if nb_components > 2:\n",
        "  min_size = 150  \n",
        "  \n",
        "  print(\"There is more components than expected\")\n",
        "\n",
        "!mkdir -p /content/styles\n",
        "def get_id(argument):\n",
        "    switcher = {\n",
        "        1: \"191u0U8owlL7ivJblZEjWHqQ5wl4UyuJA\", # royal_1\n",
        "        2: \"1K2kVzhzM0Tcs2CW9_UYVkQu1YD7ZPPrw\", # royal_2\n",
        "        3: \"11GzxCHRC2I_YinX2WnEdhOzGXU9HDxvX\", # royal_3\n",
        "        4: \"1PftlIqtWEk99h8lzS7Z_5Mv1swojg0Tx\",  # royal_4\n",
        "        5: \"1-JH6mVmJ8wAN6s3Avo-YAzP6SOcsW9la\",  # cather_5e6\n",
        "        6: \"1-JH6mVmJ8wAN6s3Avo-YAzP6SOcsW9la\",  # choosing_crop\n",
        "        7: \"1-cfb8oZuZFXGNYR6MOCyvt_v5avibB8h\",  # choosing5e5\n",
        "        8: \"1-YLl5bDLC-sskj7zrK26l7vdXqyxSJNT\",  # courage_crop\n",
        "        9: \"1-Qga7w08Lr6FWRxXagOFBha5cQymnqyy\",  # Integrity_400\n",
        "        10: \"113Um7No7tiudsrbc-o-J15B3plHN8TYc\",  # Integrity_crop\n",
        "        11: \"1-xnVWwx-KB501l-Co7o_07Vckhs7cqP8\",  # Integrity8e5\n",
        "        12: \"12i_3sVtmh68fMkFrLU3w3gyJYK2cZKlZ\",  # Interplay_m\n",
        "        13: \"10EFnY78K8AhW_UbSKZOz7MiMq0V15jJZ\",  # Interplay\n",
        "        14: \"1-U3TVDMn7SAJSaOsX8pT9YtAz3e9zOPq\",  # Justice_7e5\n",
        "        15: \"10SZLmTfskEwFnkKwFoTrUe5ljaxWjoOL\",  # Lucian2\n",
        "        16: \"1-zSLHxt8jbSByzHYU5Dxr6ntyY2_ppz7\",  # MeaningF_crop\n",
        "        17: \"12PXnOR5leAHaIf2choD9LUaQhG71QvMs\",  # MeaningF_crop2\n",
        "        18: \"10vg1ykCBXH_8XkMypWHRXRnqNUEX8sCw\",  # MeaningF_m_488\n",
        "        19: \"10Ni8oE7_BpPBjlGL-See--j9_R5aC2CU\",  # MeaningF5e5\n",
        "        20: \"11I2apRrHfKaDmJVR7KE_9PBN_PT7mMSt\",  # Mike_7e5\n",
        "        21: \"10TJfZYiWamRNRrzQvW7CO5eCpP_R1FVA\",  # Miller_7e5\n",
        "        22: \"1-ys3xyupkf9s2Fw8-0IC_X-9MsVm0HUG\",  # Miller_244\n",
        "        23: \"10-qdjq5x5Z3CY1PvqNIq_246WJofZCMF\",  # Miller_488\n",
        "        24: \"1-XwqLcJYWUMEChwBCv11wH1tbdKMsMsi\",  # Mindseye5e5\n",
        "        25: \"11Ks6rsS1rsNLQ4eg8_TeR6QDHIrPe2nj\",  # Paul_7e5\n",
        "        26: \"10Ba4vn4Md_ukC651td-POu5AFqaIm8lG\",  # Paul_crop_7e5\n",
        "        27: \"10mO5e_OvWlTOfBDVRyYTZogG-rkOJmxN\",  # Tail_7e5\n",
        "        28: \"12AZRFv6XtI3gzACSlEwBTDPXoIHNuvHn\"  # UdnieSS\n",
        "    }\n",
        "\n",
        "    return switcher.get(argument, \"Invalid Style\")\n",
        "gd_id = get_id(style_id)\n",
        "!gdown --id  $gd_id --quiet\n",
        "!gdown --id  $gd_id -O '/content/styles/$gd_id' --quiet\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import re\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def load_image(filename, size=None, scale=None):\n",
        "    img = Image.open(filename)\n",
        "    if size is not None:\n",
        "        img = img.resize((size, size), Image.ANTIALIAS)\n",
        "    elif scale is not None:\n",
        "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
        "    return img\n",
        "\n",
        "def process_image(data):\n",
        "    img = data.clone().clamp(0, 255).numpy()\n",
        "    img_arr = img.transpose(1, 2, 0).astype(\"uint8\")\n",
        "    img = Image.fromarray(img_arr)\n",
        "    return img, img_arr\n",
        "    # img.save(filename)\n",
        "# ----- Transformer Network\n",
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.tan = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "model_path = f\"/content/styles/{gd_id}\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "content_image = load_image(cropface_path)\n",
        "original_image = content_image\n",
        "\n",
        "content_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "])\n",
        "content_image = content_transform(content_image)\n",
        "content_image = content_image.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    style_model = TransformerNet()\n",
        "    state_dict = torch.load(model_path)\n",
        "    \n",
        "    for k in list(state_dict.keys()):\n",
        "        if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "            del state_dict[k]\n",
        "    style_model.load_state_dict(state_dict)\n",
        "    style_model.to(device)\n",
        "\n",
        "    output = style_model(content_image).cpu()\n",
        "\n",
        "source_st, source_st_arr = process_image(output[0])\n",
        "\n",
        "source_st = source_st.filter(ImageFilter.DETAIL)\n",
        "source_st = source_st.filter(ImageFilter.SMOOTH_MORE)\n",
        "\n",
        "# Saving the file\n",
        "# source_st.save(cropfacestyle_path)\n",
        "original_image.save(cropfacestyle_path)\n",
        "\n",
        "import skimage \n",
        "import numpy as np\n",
        "import imageio\n",
        "from skimage import io,transform,img_as_float\n",
        "from skimage.io import imread,imsave\n",
        "from PIL import Image\n",
        "from numpy import eye \n",
        "\n",
        "def match_color(target_img, source_img, eps=1e-5):\n",
        "\n",
        "    mu_t = target_img.mean(0).mean(0)\n",
        "    t = target_img - mu_t\n",
        "    t = t.transpose(2,0,1).reshape(3,-1)\n",
        "    Ct = t.dot(t.T) / t.shape[1] + eps * np.eye(t.shape[0])\n",
        "    mu_s = source_img.mean(0).mean(0)\n",
        "    s = source_img - mu_s\n",
        "    s = s.transpose(2,0,1).reshape(3,-1)\n",
        "    Cs = s.dot(s.T) / s.shape[1] + eps * np.eye(s.shape[0])\n",
        "\n",
        "    #PCA mode\n",
        "    eva_t, eve_t = np.linalg.eigh(Ct)\n",
        "    Qt = eve_t.dot(np.sqrt(np.diag(eva_t))).dot(eve_t.T)\n",
        "    eva_s, eve_s = np.linalg.eigh(Cs)\n",
        "    Qs = eve_s.dot(np.sqrt(np.diag(eva_s))).dot(eve_s.T)\n",
        "    ts = Qs.dot(np.linalg.inv(Qt)).dot(t)\n",
        "\n",
        "\n",
        "    matched_img = ts.reshape(*target_img.transpose(2,0,1).shape).transpose(1,2,0)\n",
        "    matched_img += mu_s\n",
        "    matched_img[matched_img>1] = 1\n",
        "    matched_img[matched_img<0] = 0\n",
        "    \n",
        "    return matched_img\t\n",
        "\n",
        "import skimage \n",
        "import numpy as np\n",
        "import imageio\n",
        "from skimage import io,transform,img_as_float\n",
        "from skimage.io import imread,imsave\n",
        "from PIL import Image\n",
        "from numpy import eye \n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def rgb2luv(image):\n",
        "    img = image.transpose(2,0,1).reshape(3,-1)\n",
        "    luv = np.array([[.299, .587, .114],[-.147, -.288, .436],[.615, -.515, -.1]]).dot(img).reshape((3,image.shape[0],image.shape[1]))\n",
        "    return luv.transpose(1,2,0)\n",
        "def luv2rgb(image):\n",
        "    img = image.transpose(2,0,1).reshape(3,-1)\n",
        "    rgb = np.array([[1, 0, 1.139],[1, -.395, -.580],[1, 2.03, 0]]).dot(img).reshape((3,image.shape[0],image.shape[1]))\n",
        "    return rgb.transpose(1,2,0)\n",
        "\n",
        "def doColorTransfer(org_content, output, with_color_match = False):\n",
        "  \n",
        "  org_content = imageio.imread(org_content, pilmode=\"RGB\").astype(float)/256 # original img\n",
        "  output = imageio.imread(output, pilmode=\"RGB\").astype(float)/256 # outputst\n",
        "\n",
        "\n",
        "  if with_color_match:\n",
        "    output = match_color(output, org_content)\n",
        "  \n",
        "  org_content = skimage.transform.resize(org_content, output.shape)\n",
        "\n",
        "  org_content = rgb2luv(org_content)\n",
        "  org_content[:,:,0] = output.mean(2)\n",
        "  output = luv2rgb(org_content)\n",
        "  output[output<0] = 0\n",
        "  output[output>1]=1\n",
        "\n",
        "  return output\n",
        "\n",
        "def doColorTransfer_(org_content, output, output_f = \"/content/\"):\n",
        "\n",
        "  org_content = org_content[...,::-1].astype(float)/256\n",
        "  output = output[...,::-1].astype(float)/256\n",
        "\n",
        "  org_content = cv2.resize(org_content, output.shape[:2])\n",
        "\n",
        "  org_content = rgb2luv(org_content)\n",
        "  org_content[:,:,0] = output.mean(2)\n",
        "  output = luv2rgb(org_content)\n",
        "  output[output<0] = 0\n",
        "  output[output>1]=1\n",
        "\n",
        "  return output\n",
        "\n",
        "cropface_styleCT = doColorTransfer(cropface_path, cropfacestyle_path, with_color_match = double_color_match)\n",
        "\n",
        "# Filters\n",
        "cropf_ct = Image.fromarray((cropface_styleCT * 255).astype(np.uint8))\n",
        "if do_correction_filters:\n",
        "  contrast_enh = ImageEnhance.Contrast(cropf_ct)\n",
        "  contrast_filter = contrast_enh.enhance(1.0 + contrast_)\n",
        "\n",
        "  color_enh = ImageEnhance.Color(contrast_filter)\n",
        "  color_filter = color_enh.enhance(1.0+saturation_)\n",
        "\n",
        "  br_enh = ImageEnhance.Brightness(color_filter)\n",
        "  br_filter = br_enh.enhance(1.0 +brightness_)\n",
        "\n",
        "  br_filter.save(cropfacestyleCT_path)\n",
        "\n",
        "else:\n",
        "  cropf_ct.save(cropfacestyleCT_path)\n",
        "\n",
        "def transformation_from_points(points1, points2):\n",
        "    points1 = points1.astype(np.float64)\n",
        "    points2 = points2.astype(np.float64)\n",
        "\n",
        "    c1 = np.mean(points1, axis=0)\n",
        "    c2 = np.mean(points2, axis=0)\n",
        "    points1 -= c1\n",
        "    points2 -= c2\n",
        "\n",
        "    s1 = np.std(points1)\n",
        "    s2 = np.std(points2)\n",
        "    points1 /= s1\n",
        "    points2 /= s2\n",
        "\n",
        "    U, S, Vt = np.linalg.svd(points1.T * points2)\n",
        "    R = (U * Vt).T\n",
        "\n",
        "    return np.vstack([np.hstack(((s2 / s1) * R,\n",
        "                                      c2.T - (s2 / s1) * R * c1.T)),\n",
        "                        np.matrix([0., 0., 1.])])\n",
        "def warp_im(im, M, dshape):\n",
        "    output_im = np.zeros(dshape, dtype=im.dtype)\n",
        "    cv2.warpAffine(im,\n",
        "                  M[:2],\n",
        "                  (dshape[1], dshape[0]),\n",
        "                  dst=output_im,\n",
        "                  borderMode=cv2.BORDER_TRANSPARENT,\n",
        "                  flags=cv2.WARP_INVERSE_MAP)\n",
        "    return output_im\n",
        "\n",
        "# Blurring the mask and converting it to 3 channel binary mask\n",
        "def get_masked_blur(mask, FM, kernel_s, iteration=1):\n",
        "  kernel = np.ones(kernel_s,np.uint8)\n",
        "\n",
        "  mask_im = np.array([mask,mask,mask]).transpose((1,2,0))\n",
        "  mask_im = cv2.erode(mask_im, kernel, iteration)  \n",
        "  \n",
        "\n",
        "  mask_im = (cv2.GaussianBlur(mask_im, (FM,FM), 0) > 0) * 1.0\n",
        "  mask_im = cv2.GaussianBlur(mask_im, (FM,FM), 0)\n",
        "  return mask_im  \n",
        "\n",
        "def correct_colours(im1, im2, landmarks1, COLOUR_CORRECT_BLUR_FRAC,points):\n",
        "    blur_amount = COLOUR_CORRECT_BLUR_FRAC * np.linalg.norm(\n",
        "                              np.mean(landmarks1[points[0]], axis=0) -\n",
        "                              np.mean(landmarks1[points[1]], axis=0))\n",
        "    blur_amount = int(blur_amount)\n",
        "    if blur_amount % 2 == 0:\n",
        "        blur_amount += 1\n",
        "    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n",
        "    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n",
        "\n",
        "    # Avoid divide-by-zero errors.\n",
        "    im2_blur += 128 * (im2_blur <= 1.0).astype(im2_blur.dtype)\n",
        "\n",
        "    return (im2.astype(np.float64) * im1_blur.astype(np.float64) /\n",
        "                                                im2_blur.astype(np.float64))\n",
        "\n",
        "def doTheSwap(source_path, \n",
        "              style_path, ref_path, \n",
        "              headless_path, \n",
        "              hue_value=260, \n",
        "              alpha_blend=40, \n",
        "              alpha_blend_color=40,\n",
        "              color_variation=1e-5, \n",
        "              hue_change=False, \n",
        "              morph_closing=False,\n",
        "              linear_color=False):\n",
        "\n",
        "  eps = 5e-6\n",
        "  eps *= 255*255\n",
        "  COLOUR_CORRECT_BLUR_FRAC = 0.6\n",
        "  LEFT_EYE_POINTS = list(range(42, 48))\n",
        "  RIGHT_EYE_POINTS = list(range(36, 42))\n",
        "  FACE_POINTS = list(range(17, 68))\n",
        "  LEFT_EYE_POINTS = list(range(42, 48))\n",
        "  RIGHT_EYE_POINTS = list(range(36, 42))\n",
        "  LEFT_BROW_POINTS = list(range(22, 27))\n",
        "  RIGHT_BROW_POINTS = list(range(17, 22))\n",
        "  NOSE_POINTS = list(range(27, 35))\n",
        "  MOUTH_POINTS = list(range(48, 61))\n",
        "  JAW_POINTS_ = list(range(4, 13))\n",
        "  ALIGN_POINTS = (JAW_POINTS_)\n",
        "\n",
        "  OVERLAY_POINTS = [\n",
        "      LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS+\n",
        "      NOSE_POINTS + MOUTH_POINTS+JAW_POINTS_\n",
        "  ]\n",
        "\n",
        "  FEATHER_AMOUNT = 15 \n",
        "\n",
        "  # Landmarks\n",
        "  lm = landmarks([source_path, ref_path], True)\n",
        "  source_lm = lm[0][0][0]\n",
        "  target_lm = lm[1][0][0]\n",
        "\n",
        "  # Getting the transformation matrix\n",
        "  tr = transformation_from_points(np.matrix(target_lm), np.matrix(source_lm))\n",
        "\n",
        "  # Reading inputs and showing { form-width: \"33%\" }\n",
        "  source_im = cv2.imread(source_path, cv2.IMREAD_COLOR)\n",
        "  style_im = cv2.imread(style_path, cv2.IMREAD_COLOR)\n",
        "  target_im = cv2.imread(ref_path, cv2.IMREAD_COLOR)\n",
        "  head_less = cv2.imread(headless_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "  style_linear_ct = match_color(style_im[...,::-1]/255, head_less[...,::-1]/255, eps = color_variation)\n",
        "\n",
        "  if linear_color:\n",
        "    style_im = cv2.addWeighted(style_im, alpha_blend_color/100, (style_linear_ct*255).astype(np.uint8), (1-alpha_blend_color/100),0, dtype=cv2.CV_8U)\n",
        "    style_im = style_im[...,::-1]\n",
        "\n",
        "\n",
        "  # Face segmentantion & mask of source\n",
        "  parsing = face2parsing_maps(source_path)\n",
        "  masked_face, mask_face, points_face = parsing2mask(source_im, parsing, include=[0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0]) \n",
        "  \n",
        "  mask_face = guidedFilter(source_im.astype(np.float32), mask_face.astype(np.float32), 10, eps)\n",
        "  mask_face3d = np.array([mask_face,mask_face,mask_face]).transpose(1,2,0)\n",
        "\n",
        "  #print('face mask')\n",
        "  #cv2_imshow(mask_face*255)\n",
        "  #cv2_imshow(masked_face)\n",
        "\n",
        "  \n",
        "  ## mask_face3d = get_masked_blur(mask_face, 15, (15,15)) #<- blurs and makes 3d mask\n",
        "  # masked_hair, mask_hair, points_hair = parsing2mask(source_im, parsing, include=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0])  \n",
        "\n",
        "  \n",
        "\n",
        "  # Hair mask process\n",
        "  # mask_hair_erode = cv2.erode(mask_hair, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)), iterations = 3)\n",
        "  # better_hair = guidedFilter(source_im.astype(np.float32), mask_hair_erode.astype(np.float32), 10, eps)\n",
        "  # better_hair3d = np.array([better_hair,better_hair,better_hair]).transpose(1,2,0)\n",
        "\n",
        "  # mask_blur = get_masked_blur(mask, 15, (15,15)) \n",
        "\n",
        "\n",
        "  # compose_mask =  better_hair3d + mask_face3d\n",
        "  compose_mask =  mask_face3d\n",
        "\n",
        "  # Mask morph operations\n",
        "  if morph_closing:\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10, 10))\n",
        "    compose_mask = cv2.morphologyEx(compose_mask, cv2.MORPH_CLOSE, kernel)\n",
        "    compose_mask = cv2.morphologyEx(compose_mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "  s_lm = np.matrix(source_lm[ALIGN_POINTS])\n",
        "  t_lm = np.matrix(target_lm[ALIGN_POINTS])\n",
        "  M = transformation_from_points(t_lm, s_lm) \n",
        "\n",
        "  \n",
        "  warped_mask = warp_im(compose_mask, M, target_im.shape)\n",
        "  #print('warped_mask')\n",
        "  #cv2_imshow(warped_mask*255)\n",
        "\n",
        "  # Warped style/source image\n",
        "  if hue_change:\n",
        "    # hue filter\n",
        "    hsv_img = rgb2hsv(style_im/255)\n",
        "    rgb_img = hsv2rgb(hsv_img)\n",
        "\n",
        "    hue_mean = np.mean(hsv_img[:,:,0])*360\n",
        "\n",
        "    picked_hue = 260\n",
        "    if hue_mean < picked_hue:\n",
        "      hue_shifter = (picked_hue-hue_mean)/360\n",
        "    else:\n",
        "      hue_shifter = (hue_mean-picked_hue)/360\n",
        "      hue_shifter = -hue_shifter\n",
        "    \n",
        "    new_hue = np.array([hsv_img[:,:,0]+hue_shifter, hsv_img[:,:,1], hsv_img[:,:,2]]).transpose(1,2,0)\n",
        "    \n",
        "    new_img = hsv2rgb(new_hue)\n",
        "    \n",
        "    blend_hue = cv2.addWeighted(new_img, alpha_blend/100,rgb_img, (1-alpha_blend/100),0)\n",
        "\n",
        "    warped_source_im = warp_im(blend_hue*255, M, target_im.shape)\n",
        "  else:\n",
        "    warped_source_im = warp_im(style_im, M, target_im.shape)\n",
        "    #print('warped_source_im')\n",
        "    #cv2_imshow(warped_source_im)\n",
        "\n",
        "\n",
        "\n",
        "  final = head_less*(1-warped_mask)+(warped_source_im*warped_mask)\n",
        "\n",
        "  \n",
        "  \n",
        "  print(\"Swap Done\")\n",
        "  return final\n",
        "\n",
        "\n",
        "SwapDone = doTheSwap(cropface_path, \n",
        "                    cropfacestyleCT_path, \n",
        "                    refTemp_path, \n",
        "                    hless_path, \n",
        "                    alpha_blend = hue_intensity_, \n",
        "                    alpha_blend_color=auto_color_intensity_,\n",
        "                    hue_value=hue_shifter_, \n",
        "                    hue_change=do_hue_filter,\n",
        "                    #  hue_change=False,\n",
        "                    color_variation=color_var,\n",
        "                    linear_color=do_automatic_color,\n",
        "                    morph_closing=False)\n",
        "\n",
        "\n",
        "cv2.imwrite(final_image_path, SwapDone)\n",
        "Image2 = Image.open(\"/content/output/final_image.jpg\")\n",
        "#deeplab_model = load_model()\n",
        "#foreground, bin_mask = remove_background(deeplab_model, '/content/output/final_image.jpg')\n",
        "#final_image = custom_background('/content/wb.jpg', foreground)\n",
        "#final_image.save(\"/content/final.jpg\")\n",
        "#Image2 = Image.open(\"/content/final.jpg\")\n",
        "\n",
        "image = cv2.imread(\"/content/test.jpg\")\n",
        "image_crop = Image.open(\"/content/test.jpg\")\n",
        "image_crop = image_crop.convert('RGB')\n",
        "\n",
        "\n",
        "Image2copy = Image2.copy()\n",
        "Image1copy = image_crop.copy()\n",
        "cascPath = \"/content/haarcascade_frontalface_alt2.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "final_image_p = \"/content/output/final_img.jpg\"\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "faces = faceCascade.detectMultiScale(gray,\n",
        "  scaleFactor=1.1,\n",
        "  minNeighbors=20,\n",
        "  minSize=(30, 30),\n",
        "  flags = cv2.CASCADE_SCALE_IMAGE)\n",
        "for (x, y, w, h) in faces:\n",
        "  cv2.rectangle(image, (x-w, y-h), (x+w+w, y+h+h), (0, 255, 0), 2)\n",
        "\n",
        "#Image2copy = Image2.copy()\n",
        "#Image1copy = image_crop.copy()\n",
        "Image1copy.paste(Image2copy, (x-w, y-h))\n",
        "Image1copy.save(final_image_p)\n",
        "time.sleep(0.5)\n",
        "files.download(final_image_p)"
      ],
      "metadata": {
        "id": "6a4G4SE-hOfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0z8AR7QxXARi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}